{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import dataloader\n",
    "from models import main_models\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# parser=argparse.ArgumentParser()\n",
    "# parser.add_argument('--n_epoches_1',type=int,default=10)\n",
    "# parser.add_argument('--n_epoches_2',type=int,default=100)\n",
    "# parser.add_argument('--n_epoches_3',type=int,default=100)\n",
    "# parser.add_argument('--n_target_samples',type=int,default=7)\n",
    "# parser.add_argument('--batch_size',type=int,default=64)\n",
    "\n",
    "# opt=vars(parser.parse_args())\n",
    "opt = {}\n",
    "opt['n_epoches_1'] = 10\n",
    "opt['n_epoches_2'] = 100\n",
    "opt['n_epoches_3'] = 100\n",
    "opt['n_target_samples'] = 7\n",
    "opt['batch_size'] = 64\n",
    "\n",
    "\n",
    "use_cuda=True if torch.cuda.is_available() else False\n",
    "device=torch.device('cuda:0') if use_cuda else torch.device('cpu')\n",
    "torch.manual_seed(1)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "\n",
    "#--------------pretrain g and h for step 1---------------------------------\n",
    "train_dataloader=dataloader.mnist_dataloader(batch_size=opt['batch_size'],train=True)\n",
    "test_dataloader=dataloader.mnist_dataloader(batch_size=opt['batch_size'],train=False)\n",
    "\n",
    "classifier=main_models.Classifier()\n",
    "encoder=main_models.Encoder()\n",
    "discriminator=main_models.DCD(input_features=128)\n",
    "\n",
    "classifier.to(device)\n",
    "encoder.to(device)\n",
    "discriminator.to(device)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer=torch.optim.Adam(list(encoder.parameters())+list(classifier.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1----Epoch 1/10  accuracy: 0.969 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cd01982a3dc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_epoches_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Pytorch_test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Pytorch_test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Pytorch_test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Pytorch_test\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Pytorch_test\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Pytorch_test\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Pytorch_test\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(opt['n_epoches_1']):\n",
    "    for data,labels in train_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred=classifier(encoder(data))\n",
    "\n",
    "        loss=loss_fn(y_pred,labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    acc=0\n",
    "    for data,labels in test_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        y_test_pred=classifier(encoder(data))\n",
    "        acc+=(torch.max(y_test_pred,1)[1]==labels).float().mean().item()\n",
    "\n",
    "    accuracy=round(acc / float(len(test_dataloader)), 3)\n",
    "\n",
    "    print(\"step1----Epoch %d/%d  accuracy: %.3f \"%(epoch+1,opt['n_epoches_1'],accuracy))\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "feature_vector = []\n",
    "labels_vector = []\n",
    "for data,labels in test_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        y_test_pred=classifier(encoder(data))\n",
    "        for i in range(len(labels)):\n",
    "            feature_vector.append(y_test_pred[i].cpu().detach().tolist())\n",
    "            labels_vector.append(labels[i].cpu().detach().tolist())\n",
    "#         print (y_test_pred.shape)\n",
    "#         feature_vector += y_test_pred.cpu().detach().numpy()\n",
    "#         labels_vector += labels.cpu().detach().numpy()\n",
    "        \n",
    "#         sys.exit()\n",
    "print (np.array(labels_vector).shape)\n",
    "print (np.array(feature_vector).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# We import sklearn.\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# We'll hack a bit with the t-SNE code in sklearn 0.15.2.\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.manifold.t_sne import (_joint_probabilities,\n",
    "                                    _kl_divergence)\n",
    "\n",
    "# Random state.\n",
    "RS = 20150101\n",
    "no = 200\n",
    "new_feature_stack = np.array(feature_vector[:no] + feature_vector[-no:])\n",
    "    \n",
    "print (len(new_feature_stack))\n",
    "digits_proj = TSNE(random_state=RS).fit_transform(new_feature_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 10)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "new_labels_vector = np.array(labels_vector[:no] + labels_vector[-no:])\n",
    "print (new_feature_stack.shape)\n",
    "print (new_labels_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We import seaborn to make nice plots.\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "# We'll use matplotlib for graphics.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(10):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts\n",
    "scatter(digits_proj, new_labels_vector)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/SVHN\\train_32x32.mat\n",
      "Sampling groups\n",
      "step2----Epoch 1/100 loss:1.370\n",
      "Sampling groups\n",
      "step2----Epoch 2/100 loss:1.288\n",
      "Sampling groups\n",
      "step2----Epoch 3/100 loss:1.232\n",
      "Sampling groups\n",
      "step2----Epoch 4/100 loss:1.181\n",
      "Sampling groups\n",
      "step2----Epoch 5/100 loss:1.141\n",
      "Sampling groups\n",
      "step2----Epoch 6/100 loss:1.098\n",
      "Sampling groups\n",
      "step2----Epoch 7/100 loss:1.089\n",
      "Sampling groups\n",
      "step2----Epoch 8/100 loss:1.050\n",
      "Sampling groups\n",
      "step2----Epoch 9/100 loss:1.045\n",
      "Sampling groups\n",
      "step2----Epoch 10/100 loss:1.045\n",
      "Sampling groups\n",
      "step2----Epoch 11/100 loss:1.040\n",
      "Sampling groups\n",
      "step2----Epoch 12/100 loss:1.023\n",
      "Sampling groups\n",
      "step2----Epoch 13/100 loss:1.031\n",
      "Sampling groups\n",
      "step2----Epoch 14/100 loss:1.029\n",
      "Sampling groups\n",
      "step2----Epoch 15/100 loss:1.034\n",
      "Sampling groups\n",
      "step2----Epoch 16/100 loss:1.002\n",
      "Sampling groups\n",
      "step2----Epoch 17/100 loss:1.012\n",
      "Sampling groups\n",
      "step2----Epoch 18/100 loss:1.003\n",
      "Sampling groups\n",
      "step2----Epoch 19/100 loss:1.029\n",
      "Sampling groups\n",
      "step2----Epoch 20/100 loss:1.000\n",
      "Sampling groups\n",
      "step2----Epoch 21/100 loss:1.011\n",
      "Sampling groups\n",
      "step2----Epoch 22/100 loss:0.996\n",
      "Sampling groups\n",
      "step2----Epoch 23/100 loss:0.980\n",
      "Sampling groups\n",
      "step2----Epoch 24/100 loss:0.993\n",
      "Sampling groups\n",
      "step2----Epoch 25/100 loss:1.014\n",
      "Sampling groups\n",
      "step2----Epoch 26/100 loss:1.020\n",
      "Sampling groups\n",
      "step2----Epoch 27/100 loss:0.989\n",
      "Sampling groups\n",
      "step2----Epoch 28/100 loss:0.988\n",
      "Sampling groups\n",
      "step2----Epoch 29/100 loss:0.979\n",
      "Sampling groups\n",
      "step2----Epoch 30/100 loss:0.986\n",
      "Sampling groups\n",
      "step2----Epoch 31/100 loss:1.034\n",
      "Sampling groups\n",
      "step2----Epoch 32/100 loss:0.990\n",
      "Sampling groups\n",
      "step2----Epoch 33/100 loss:0.966\n",
      "Sampling groups\n",
      "step2----Epoch 34/100 loss:0.970\n",
      "Sampling groups\n",
      "step2----Epoch 35/100 loss:0.995\n",
      "Sampling groups\n",
      "step2----Epoch 36/100 loss:0.960\n",
      "Sampling groups\n",
      "step2----Epoch 37/100 loss:0.974\n",
      "Sampling groups\n",
      "step2----Epoch 38/100 loss:0.980\n",
      "Sampling groups\n",
      "step2----Epoch 39/100 loss:0.969\n",
      "Sampling groups\n",
      "step2----Epoch 40/100 loss:0.994\n",
      "Sampling groups\n",
      "step2----Epoch 41/100 loss:0.981\n",
      "Sampling groups\n",
      "step2----Epoch 42/100 loss:0.996\n",
      "Sampling groups\n",
      "step2----Epoch 43/100 loss:0.984\n",
      "Sampling groups\n",
      "step2----Epoch 44/100 loss:0.964\n",
      "Sampling groups\n",
      "step2----Epoch 45/100 loss:0.977\n",
      "Sampling groups\n",
      "step2----Epoch 46/100 loss:1.008\n",
      "Sampling groups\n",
      "step2----Epoch 47/100 loss:0.965\n",
      "Sampling groups\n",
      "step2----Epoch 48/100 loss:0.947\n",
      "Sampling groups\n",
      "step2----Epoch 49/100 loss:0.944\n",
      "Sampling groups\n",
      "step2----Epoch 50/100 loss:0.948\n",
      "Sampling groups\n",
      "step2----Epoch 51/100 loss:0.974\n",
      "Sampling groups\n",
      "step2----Epoch 52/100 loss:0.961\n",
      "Sampling groups\n",
      "step2----Epoch 53/100 loss:0.936\n",
      "Sampling groups\n",
      "step2----Epoch 54/100 loss:0.949\n",
      "Sampling groups\n",
      "step2----Epoch 55/100 loss:0.979\n",
      "Sampling groups\n",
      "step2----Epoch 56/100 loss:0.956\n",
      "Sampling groups\n",
      "step2----Epoch 57/100 loss:0.971\n",
      "Sampling groups\n",
      "step2----Epoch 58/100 loss:0.956\n",
      "Sampling groups\n",
      "step2----Epoch 59/100 loss:0.947\n",
      "Sampling groups\n",
      "step2----Epoch 60/100 loss:0.966\n",
      "Sampling groups\n",
      "step2----Epoch 61/100 loss:0.957\n",
      "Sampling groups\n",
      "step2----Epoch 62/100 loss:0.940\n",
      "Sampling groups\n",
      "step2----Epoch 63/100 loss:0.947\n",
      "Sampling groups\n",
      "step2----Epoch 64/100 loss:0.976\n",
      "Sampling groups\n",
      "step2----Epoch 65/100 loss:0.943\n",
      "Sampling groups\n",
      "step2----Epoch 66/100 loss:0.965\n",
      "Sampling groups\n",
      "step2----Epoch 67/100 loss:0.947\n",
      "Sampling groups\n",
      "step2----Epoch 68/100 loss:0.991\n",
      "Sampling groups\n",
      "step2----Epoch 69/100 loss:0.946\n",
      "Sampling groups\n",
      "step2----Epoch 70/100 loss:0.969\n",
      "Sampling groups\n",
      "step2----Epoch 71/100 loss:0.922\n",
      "Sampling groups\n",
      "step2----Epoch 72/100 loss:0.908\n",
      "Sampling groups\n",
      "step2----Epoch 73/100 loss:0.936\n",
      "Sampling groups\n",
      "step2----Epoch 74/100 loss:0.980\n",
      "Sampling groups\n",
      "step2----Epoch 75/100 loss:0.927\n",
      "Sampling groups\n",
      "step2----Epoch 76/100 loss:0.929\n",
      "Sampling groups\n",
      "step2----Epoch 77/100 loss:0.975\n",
      "Sampling groups\n",
      "step2----Epoch 78/100 loss:0.966\n",
      "Sampling groups\n",
      "step2----Epoch 79/100 loss:0.947\n",
      "Sampling groups\n",
      "step2----Epoch 80/100 loss:0.979\n",
      "Sampling groups\n",
      "step2----Epoch 81/100 loss:0.934\n",
      "Sampling groups\n",
      "step2----Epoch 82/100 loss:0.963\n",
      "Sampling groups\n",
      "step2----Epoch 83/100 loss:0.935\n",
      "Sampling groups\n",
      "step2----Epoch 84/100 loss:0.941\n",
      "Sampling groups\n",
      "step2----Epoch 85/100 loss:0.917\n",
      "Sampling groups\n",
      "step2----Epoch 86/100 loss:0.953\n",
      "Sampling groups\n",
      "step2----Epoch 87/100 loss:0.975\n",
      "Sampling groups\n",
      "step2----Epoch 88/100 loss:0.959\n",
      "Sampling groups\n",
      "step2----Epoch 89/100 loss:0.981\n",
      "Sampling groups\n",
      "step2----Epoch 90/100 loss:0.962\n",
      "Sampling groups\n",
      "step2----Epoch 91/100 loss:0.959\n",
      "Sampling groups\n",
      "step2----Epoch 92/100 loss:0.939\n",
      "Sampling groups\n",
      "step2----Epoch 93/100 loss:0.919\n",
      "Sampling groups\n",
      "step2----Epoch 94/100 loss:0.949\n",
      "Sampling groups\n",
      "step2----Epoch 95/100 loss:0.932\n",
      "Sampling groups\n",
      "step2----Epoch 96/100 loss:0.918\n",
      "Sampling groups\n",
      "step2----Epoch 97/100 loss:0.956\n",
      "Sampling groups\n",
      "step2----Epoch 98/100 loss:0.935\n",
      "Sampling groups\n",
      "step2----Epoch 99/100 loss:0.941\n",
      "Sampling groups\n",
      "step2----Epoch 100/100 loss:0.942\n"
     ]
    }
   ],
   "source": [
    "X_s,Y_s=dataloader.sample_data()\n",
    "X_t,Y_t=dataloader.create_target_samples(opt['n_target_samples'])\n",
    "\n",
    "#-----------------train DCD for step 2--------------------------------\n",
    "\n",
    "optimizer_D=torch.optim.Adam(discriminator.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(opt['n_epoches_2']):\n",
    "    # data\n",
    "    groups,aa = dataloader.sample_groups(X_s,Y_s,X_t,Y_t,seed=epoch)\n",
    "\n",
    "    n_iters = 4 * len(groups[1])\n",
    "    index_list = torch.randperm(n_iters)\n",
    "    mini_batch_size=40 #use mini_batch train can be more stable\n",
    "\n",
    "\n",
    "    loss_mean=[]\n",
    "\n",
    "    X1=[];X2=[];ground_truths=[]\n",
    "    for index in range(n_iters):\n",
    "\n",
    "        ground_truth=index_list[index]//len(groups[1])\n",
    "\n",
    "        x1,x2=groups[ground_truth][index_list[index]-len(groups[1])*ground_truth]\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths.append(ground_truth)\n",
    "\n",
    "        #select data for a mini-batch to train\n",
    "        if (index+1)%mini_batch_size==0:\n",
    "            X1=torch.stack(X1)\n",
    "            X2=torch.stack(X2)\n",
    "            ground_truths=torch.LongTensor(ground_truths)\n",
    "            X1=X1.to(device)\n",
    "            X2=X2.to(device)\n",
    "            ground_truths=ground_truths.to(device)\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            X_cat=torch.cat([encoder(X1),encoder(X2)],1)\n",
    "            y_pred=discriminator(X_cat.detach())\n",
    "            loss=loss_fn(y_pred,ground_truths)\n",
    "            loss.backward()\n",
    "            optimizer_D.step()\n",
    "            loss_mean.append(loss.item())\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths = []\n",
    "\n",
    "    print(\"step2----Epoch %d/%d loss:%.3f\"%(epoch+1,opt['n_epoches_2'],np.mean(loss_mean)))\n",
    "\n",
    "#----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/SVHN\\test_32x32.mat\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n",
      "Sampling groups\n"
     ]
    }
   ],
   "source": [
    "#-------------------training for step 3-------------------\n",
    "optimizer_g_h=torch.optim.Adam(list(encoder.parameters())+list(classifier.parameters()),lr=0.001)\n",
    "optimizer_d=torch.optim.Adam(discriminator.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "test_dataloader=dataloader.svhn_dataloader(train=False,batch_size=opt['batch_size'])\n",
    "\n",
    "\n",
    "for epoch in range(opt['n_epoches_3']):\n",
    "    #---training g and h , DCD is frozen\n",
    "\n",
    "    groups, groups_y = dataloader.sample_groups(X_s,Y_s,X_t,Y_t,seed=opt['n_epoches_2']+epoch)\n",
    "    G1, G2, G3, G4 = groups\n",
    "    Y1, Y2, Y3, Y4 = groups_y\n",
    "    groups_2 = [G2, G4]\n",
    "    groups_y_2 = [Y2, Y4]\n",
    "\n",
    "    n_iters = 2 * len(G2)\n",
    "    index_list = torch.randperm(n_iters)\n",
    "\n",
    "    n_iters_dcd = 4 * len(G2)\n",
    "    index_list_dcd = torch.randperm(n_iters_dcd)\n",
    "\n",
    "    mini_batch_size_g_h = 20 #data only contains G2 and G4 ,so decrease mini_batch\n",
    "    mini_batch_size_dcd= 40 #data contains G1,G2,G3,G4 so use 40 as mini_batch\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    ground_truths_y1 = []\n",
    "    ground_truths_y2 = []\n",
    "    dcd_labels=[]\n",
    "    for index in range(n_iters):\n",
    "\n",
    "\n",
    "        ground_truth=index_list[index]//len(G2)\n",
    "        x1, x2 = groups_2[ground_truth][index_list[index] - len(G2) * ground_truth]\n",
    "        y1, y2 = groups_y_2[ground_truth][index_list[index] - len(G2) * ground_truth]\n",
    "        # y1=torch.LongTensor([y1.item()])\n",
    "        # y2=torch.LongTensor([y2.item()])\n",
    "        dcd_label=0 if ground_truth==0 else 2\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths_y1.append(y1)\n",
    "        ground_truths_y2.append(y2)\n",
    "        dcd_labels.append(dcd_label)\n",
    "\n",
    "        if (index+1)%mini_batch_size_g_h==0:\n",
    "\n",
    "            X1=torch.stack(X1)\n",
    "            X2=torch.stack(X2)\n",
    "            ground_truths_y1=torch.LongTensor(ground_truths_y1)\n",
    "            ground_truths_y2 = torch.LongTensor(ground_truths_y2)\n",
    "            dcd_labels=torch.LongTensor(dcd_labels)\n",
    "            X1=X1.to(device)\n",
    "            X2=X2.to(device)\n",
    "            ground_truths_y1=ground_truths_y1.to(device)\n",
    "            ground_truths_y2 = ground_truths_y2.to(device)\n",
    "            dcd_labels=dcd_labels.to(device)\n",
    "\n",
    "            optimizer_g_h.zero_grad()\n",
    "\n",
    "            encoder_X1=encoder(X1)\n",
    "            encoder_X2=encoder(X2)\n",
    "\n",
    "            X_cat=torch.cat([encoder_X1,encoder_X2],1)\n",
    "            y_pred_X1=classifier(encoder_X1)\n",
    "            y_pred_X2=classifier(encoder_X2)\n",
    "            y_pred_dcd=discriminator(X_cat)\n",
    "\n",
    "            loss_X1=loss_fn(y_pred_X1,ground_truths_y1)\n",
    "            loss_X2=loss_fn(y_pred_X2,ground_truths_y2)\n",
    "            loss_dcd=loss_fn(y_pred_dcd,dcd_labels)\n",
    "\n",
    "            loss_sum = loss_X1 + loss_X2 + 0.2 * loss_dcd\n",
    "\n",
    "            loss_sum.backward()\n",
    "            optimizer_g_h.step()\n",
    "\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths_y1 = []\n",
    "            ground_truths_y2 = []\n",
    "            dcd_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step3----Epoch 100/100  accuracy: 0.470 \n"
     ]
    }
   ],
   "source": [
    "    #----training dcd ,g and h frozen\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    ground_truths = []\n",
    "    for index in range(n_iters_dcd):\n",
    "\n",
    "        ground_truth=index_list_dcd[index]//len(groups[1])\n",
    "\n",
    "        x1, x2 = groups[ground_truth][index_list_dcd[index] - len(groups[1]) * ground_truth]\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths.append(ground_truth)\n",
    "\n",
    "        if (index + 1) % mini_batch_size_dcd == 0:\n",
    "            X1 = torch.stack(X1)\n",
    "            X2 = torch.stack(X2)\n",
    "            ground_truths = torch.LongTensor(ground_truths)\n",
    "            X1 = X1.to(device)\n",
    "            X2 = X2.to(device)\n",
    "            ground_truths = ground_truths.to(device)\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "            X_cat = torch.cat([encoder(X1), encoder(X2)], 1)\n",
    "            y_pred = discriminator(X_cat.detach())\n",
    "            loss = loss_fn(y_pred, ground_truths)\n",
    "            loss.backward()\n",
    "            optimizer_d.step()\n",
    "            # loss_mean.append(loss.item())\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths = []\n",
    "\n",
    "    #testing\n",
    "    acc = 0\n",
    "    for data, labels in test_dataloader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_test_pred = classifier(encoder(data))\n",
    "        acc += (torch.max(y_test_pred, 1)[1] == labels).float().mean().item()\n",
    "\n",
    "    accuracy = round(acc / float(len(test_dataloader)), 3)\n",
    "\n",
    "    print(\"step3----Epoch %d/%d  accuracy: %.3f \" % (epoch + 1, opt['n_epoches_3'], accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
